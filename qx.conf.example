# QX Configuration Example
# Place this file in one of the following locations (in order of priority):
# 1. /etc/qx/qx.conf (system-wide, lowest priority)
# 2. ~/.config/qx/qx.conf (user-level)
# 3. <project-directory>/.Q/qx.conf (project-level, highest priority)

# === REQUIRED SETTINGS ===

# Model Configuration - Use LiteLLM format
# Format: provider/model-name or provider/organization/model-name
QX_MODEL_NAME=openrouter/anthropic/claude-3.5-sonnet

# API Keys (choose based on your provider)
# For OpenRouter
OPENROUTER_API_KEY=sk-or-v1-your_openrouter_api_key_here

# For OpenAI (if using OpenAI directly)
# OPENAI_API_KEY=sk-your_openai_api_key_here

# For Anthropic (if using Anthropic directly)
# ANTHROPIC_API_KEY=sk-ant-your_anthropic_api_key_here

# For Azure OpenAI
# AZURE_API_KEY=your_azure_api_key_here
# AZURE_API_BASE=https://your-resource.openai.azure.com/
# AZURE_API_VERSION=2024-02-01

# === OPTIONAL SETTINGS ===

# Model Parameters
QX_MODEL_TEMPERATURE=0.7
QX_MODEL_MAX_TOKENS=4096
QX_MODEL_REASONING_EFFORT=medium  # Options: low, medium, high (for reasoning models)

# Provider Configuration (LiteLLM specific)
# Specify preferred provider order (comma-separated)
QX_MODEL_PROVIDER=OpenRouter,OpenAI,Anthropic

# Allow fallback to other providers if primary fails
QX_ALLOW_PROVIDER_FALLBACK=true

# Streaming Configuration
QX_ENABLE_STREAMING=true

# === RELIABILITY & RESILIENCE SETTINGS ===

# Retry Configuration
QX_NUM_RETRIES=3              # Number of retry attempts for failed requests
QX_RETRY_DELAY=1.0            # Base delay between retries in seconds
QX_MAX_RETRY_DELAY=60.0       # Maximum delay between retries
QX_BACKOFF_FACTOR=2.0         # Exponential backoff multiplier

# Fallback Configuration  
QX_FALLBACK_MODELS=gpt-4o,claude-3-5-sonnet-20241022,gemini-1.5-pro
QX_FALLBACK_TIMEOUT=45        # Total timeout for fallback attempts (seconds)
QX_FALLBACK_COOLDOWN=60       # Cooldown period for rate-limited models (seconds)

# Context Window Fallback Mapping (JSON format)
# Maps models to their larger context equivalents
QX_CONTEXT_WINDOW_FALLBACKS={"gpt-3.5-turbo":"gpt-3.5-turbo-16k","gpt-4":"gpt-4-32k","claude-3-haiku":"claude-3-sonnet"}

# Multiple API Key Fallbacks (JSON format)
# Backup API keys for the same provider
QX_FALLBACK_API_KEYS={"openai":["sk-backup1","sk-backup2"],"anthropic":["sk-ant-backup1"]}

# Multiple API Base Fallbacks (JSON format)  
# Backup API endpoints for the same provider
QX_FALLBACK_API_BASES={"azure":[{"api_key":"key1","api_base":"https://backup1.azure.com"},{"api_key":"key2","api_base":"https://backup2.azure.com"}]}

# Circuit Breaker Settings
QX_CIRCUIT_BREAKER_ENABLED=true
QX_CIRCUIT_BREAKER_THRESHOLD=5    # Number of failures before opening circuit
QX_CIRCUIT_BREAKER_TIMEOUT=300    # Circuit breaker timeout in seconds

# Request Timeout Settings
QX_REQUEST_TIMEOUT=120            # Individual request timeout (seconds)
QX_CONNECT_TIMEOUT=30             # Connection timeout (seconds)
QX_READ_TIMEOUT=300               # Read timeout for streaming (seconds)

# Logging Configuration
QX_LOG_LEVEL=INFO  # Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
QX_LOG_SENT=false  # Log messages sent to LLM
QX_LOG_RECEIVED=false  # Log responses from LLM
QX_DEBUG_STREAMING=false  # Debug streaming issues

# === EXAMPLE MODEL CONFIGURATIONS ===

# OpenRouter Models (recommended - access to many providers)
# QX_MODEL_NAME=openrouter/openai/gpt-4o
# QX_MODEL_NAME=openrouter/anthropic/claude-3.5-sonnet
# QX_MODEL_NAME=openrouter/google/gemini-2.5-flash-preview-05-20
# QX_MODEL_NAME=openrouter/meta-llama/llama-3.1-405b-instruct

# Direct Provider Models  
# QX_MODEL_NAME=gpt-4o                    # OpenAI direct
# QX_MODEL_NAME=claude-3-5-sonnet-20241022 # Anthropic direct
# QX_MODEL_NAME=vertex_ai/gemini-1.5-pro  # Google Vertex AI
# QX_MODEL_NAME=azure/gpt-4o              # Azure OpenAI

# Local Models
# QX_MODEL_NAME=ollama/llama3.2
# QX_MODEL_NAME=vllm/meta-llama/Llama-3.2-3B-Instruct

# === COST TRACKING & OBSERVABILITY ===

# LiteLLM Observability (optional)
# LANGFUSE_PUBLIC_KEY=pk-lf-your_langfuse_public_key
# LANGFUSE_SECRET_KEY=sk-lf-your_langfuse_secret_key
# HELICONE_API_KEY=sk-helicone-your_helicone_api_key
# LUNARY_PUBLIC_KEY=your_lunary_public_key

# === ENVIRONMENT-SPECIFIC SETTINGS ===

# Project Context (automatically loaded if present)
# QX_PROJECT_CONTEXT=  # Loaded from .Q/project.md
# QX_USER_CONTEXT=     # Loaded from ~/.config/qx/user.md
# QX_PROJECT_FILES=    # Auto-generated project file tree

# Performance Tuning (deprecated - use reliability settings above)
# QX_HTTP_TIMEOUT=300  # Use QX_REQUEST_TIMEOUT instead
# QX_MAX_RETRIES=3     # Use QX_NUM_RETRIES instead
# QX_RETRY_DELAY=1     # Use QX_RETRY_DELAY instead

# === ADVANCED RELIABILITY EXAMPLES ===

# Example 1: High Availability Setup
# QX_FALLBACK_MODELS=openrouter/openai/gpt-4o,openrouter/anthropic/claude-3.5-sonnet,openrouter/google/gemini-1.5-pro
# QX_NUM_RETRIES=5
# QX_FALLBACK_TIMEOUT=60

# Example 2: Cost-Optimized with Fallbacks  
# QX_MODEL_NAME=openrouter/meta-llama/llama-3.1-8b-instruct
# QX_FALLBACK_MODELS=openrouter/openai/gpt-3.5-turbo,openrouter/openai/gpt-4o-mini

# Example 3: Context Window Auto-Scaling
# QX_MODEL_NAME=gpt-3.5-turbo
# QX_CONTEXT_WINDOW_FALLBACKS={"gpt-3.5-turbo":"gpt-3.5-turbo-16k","gpt-4":"gpt-4-32k"}

# Example 4: Multi-Region Azure Setup
# QX_MODEL_NAME=azure/gpt-4
# QX_FALLBACK_API_BASES={"azure":[{"api_key":"key1","api_base":"https://east.openai.azure.com"},{"api_key":"key2","api_base":"https://west.openai.azure.com"}]}